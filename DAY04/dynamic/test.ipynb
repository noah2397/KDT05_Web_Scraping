{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "print(selenium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://google.com')\n",
    "#driver.implicitly_wait(3)\n",
    "search_box = driver.find_element(By.NAME, 'q') # 검색창을 찾은 후에,\n",
    "search_box.send_keys('Python') # Python이라고 치고,\n",
    "search_box.submit() # 검색 버튼 누름\n",
    "time.sleep(3) # 3초간 대기....\n",
    "search_results = driver.find_elements(By.CSS_SELECTOR, \"div.g\") # 결과창이 있는 박스를 여러개 찾음\n",
    "print(len(search_results)) # 박스의 길이 출력\n",
    "# Extract and print the title and URL of each search result\n",
    "for result in search_results: # 여러 요소를 뽑았으니 반복문 출력\n",
    "    title_element = result.find_element(By.CSS_SELECTOR, \"h3\") # h3인 요소를 뽑아내서\n",
    "    title = title_element.text.strip()\n",
    "    print(f\"{title}\") # 출력\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome() # 본인의 webdriver 경로\n",
    "driver.get('https://blog.naver.com/swf1004/221631056531')\n",
    "driver.switch_to.frame('mainFrame') # 해당 iframe으로 이동(id 참조)\n",
    "html = driver.page_source # beautifulSoup와 연계\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "whole_border = soup.find('div', {'id': 'whole-border'})\n",
    "results = whole_border.find_all('div', {'class': 'se-module'})\n",
    "result1=[]\n",
    "for result in results:\n",
    "    print(result.text.replace('\\n', '')) # 줄바꿈을 없애기\n",
    "    result1.append(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.coffeebeankorea.com/store/store.asp')\n",
    "driver.execute_script('storePop2(31)')\n",
    "html = driver.page_source # page_source: 해당 웹페이지의 소스가 저장됨\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup.prettify()) # HTML 소스를 보기 좋게 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.coffeebeankorea.com/store/store.asp')\n",
    "# 팝업창 생성됨\n",
    "\n",
    "# 현재의 html 소스를 저장\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup.prettify()) # HTML 소스를 보기 좋게 출력\n",
    "store_names = soup.select('div.store_txt > p.name > span')\n",
    "store_name_list = []\n",
    "for name in store_names:\n",
    "    store_name_list.append(name.get_text())\n",
    "print('매장 개수: ', len(store_name_list))\n",
    "print(store_name_list)\n",
    "store_addresses = soup.select('p.address > span')\n",
    "store_address_list = []\n",
    "for addr in store_addresses:\n",
    "    print(addr.get_text())\n",
    "    store_address_list.append(addr.get_text())\n",
    "driver.quit() # web driver 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "def coffeebean_store(store_list):\n",
    "    coffeebean_url = 'https://www.coffeebeankorea.com/store/store.asp'\n",
    "    driver = webdriver.Chrome()\n",
    "    for i in range(1, 388): # 매장 개수만큼 반복한다\n",
    "        driver.get(coffeebean_url)\n",
    "        time.sleep(1) # 웹페이지를 연결할 동안 1초 대기\n",
    "        driver.execute_script('storePop2(%d)' % i)\n",
    "\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            store_name = soup.select_one('div.store_txt > h2').text # 매장 이름\n",
    "\n",
    "            store_info = soup.select(\"div.store_txt > table.store_table > tbody > tr > td\")\n",
    "\n",
    "            store_address_list = list(store_info[2])\n",
    "            store_addr = store_address_list[0] # 매장 주소\n",
    "            store_phone = store_info[3].text # 매장 전화번호\n",
    "            print('{} {} {}'.format(i+1, store_name, store_addr, store_phone))\n",
    "\n",
    "            store_list.append([store_name, store_addr, store_phone])\n",
    "        except:\n",
    "            continue\n",
    "def main():\n",
    "    store_info = []\n",
    "    coffeebean_store(store_info)\n",
    "    # DataFrame으로 변경\n",
    "    coffeebean_table = pd.DataFrame(store_info, columns=('매장이름', '주소', '전화번호'))\n",
    "    print(coffeebean_table.head())\n",
    "    # DataFrame을 csv파일로 저장 (utf-8로 인코딩)\n",
    "    coffeebean_table.to_csv('coffeebean_branches.csv', encoding='utf-8', mode='w', index=True)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 검색 API 예제 - 블로그 검색\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "client_id = \"ebU_O6PVpe7hodK5LVTM\" # 발급받은 ID 입력\n",
    "client_secret = \"Ih1p91C2tP\" # 발급받은 PASSWORD 입력\n",
    "encText = urllib.parse.quote(\"취업준비\")\n",
    "url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText # JSON 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # XML 결과\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    print(response_body.decode('utf-8'))\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import datetime\n",
    "import json\n",
    "def get_request_url(url):\n",
    "    client_id = \"ebU_O6PVpe7hodK5LVTM\"\n",
    "    client_secret = \"5hdXOiyl6N\"\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    req.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req) # url 요청\n",
    "        if response.getcode() == 200:\n",
    "            return response.read().decode('utf-8') # 읽어들인 문자열을 반환\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error for URL: {url}\")\n",
    "        \n",
    "def get_naver_search(node, search_text, start, display):\n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = f\"/{node}.json\" # node : 크롤링 대상\n",
    "    query_string = f\"{urllib.parse.quote(search_text)}\"\n",
    "    # f\"?query={query_string}&start={start}&display={display}\"\n",
    "    parameters = (\"?query={}&start={}&display={}\". \n",
    "    format(query_string, start, display))\n",
    "    url = base + node + parameters\n",
    "    response = get_request_url(url) # 하나의 url완성\n",
    "    if response is None:\n",
    "        return None\n",
    "    else:\n",
    "        # json 문자열을 Python 객체로 변환\n",
    "        return json.loads(response)\n",
    "    \n",
    "def main():\n",
    "    node = 'news' # 크롤링 대상\n",
    "    # search_text = input('검색어를 입력하세요: ')\n",
    "    search_text = '휴학'\n",
    "    cnt = 0\n",
    "    json_response = get_naver_search(node, search_text, 1, 100)\n",
    "    if (json_response is not None) and (json_response['display'] != 0):\n",
    "        for post in json_response['items']:\n",
    "            cnt += 1\n",
    "            # 1단계\n",
    "\n",
    "            print(f\"[{cnt}]\", end=\" \")\n",
    "            print(post['title'])\n",
    "            print(post['description'])\n",
    "            print(post['originallink'])\n",
    "            print(post['link'])\n",
    "            print(post['pubDate'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_string = \"Sat, 10 Feb 2024 09:02:00 +0900\"\n",
    "# strptime(string, format): string -> datetime 변환\n",
    "pdate = datetime.datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S +0900')\n",
    "print(type(pdate))\n",
    "# strftime(format): datetime -> string 변환\n",
    "pdate_string = pdate.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(type(pdate_string))\n",
    "print(pdate_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import datetime\n",
    "import json\n",
    "def get_request_url(url):\n",
    "    client_id = \"ebU_O6PVpe7hodK5LVTM\"\n",
    "    client_secret = \"5hdXOiyl6N\"\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    req.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "        if response.getcode() == 200:\n",
    "            return response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error for URL: {url}\")\n",
    "        \n",
    "def get_naver_search(node, search_text, start, display):\n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = f\"/{node}.json\"\n",
    "    query_string = f\"{urllib.parse.quote(search_text)}\"\n",
    "    # f\"?query={query_string}&start={start}&display={display}\"\n",
    "    parameters = (\"?query={}&start={}&display={}\".\n",
    "    format(query_string, start, display))\n",
    "    url = base + node + parameters\n",
    "    response = get_request_url(url)\n",
    "    if response is None:\n",
    "        return None\n",
    "    else:\n",
    "        # json 문자열을 Python 객체로 변환\n",
    "        return json.loads(response)\n",
    "    \n",
    "def get_post_data(post, json_result_list, cnt): # 데이터들을 여기서 뽑음\n",
    "    title = post['title']\n",
    "    description = post['description']\n",
    "    org_link = post['originallink']\n",
    "    link = post['link']\n",
    "    '''\n",
    "    strptime()\n",
    "    - %a: abbreviated weekday name\n",
    "    - %b\" abbreviated month name\n",
    "    '''\n",
    "    pdate = datetime.datetime.strptime(post['pubDate'], '%a, %d %b %Y %H:%M:%S +0900') # strptime으로 알맞은 형식에 parsing한 다음에,\n",
    "    pdate = pdate.strftime('%Y-%m-%d %H:%M:%S') # 최종적으로 시간 형식을 맞춤\n",
    "    print(f\"[{cnt}]\", end=\" \")\n",
    "    print(title, end=\": \")\n",
    "    print(pdate, end=\" \")  \n",
    "    print(link)\n",
    "    #                  ['번호', '날짜', '제목', '개요', '원본기사링크', '네이버링크']\n",
    "    json_result_list.append([cnt, pdate, title, description, org_link, link])\n",
    "\n",
    "def main():\n",
    "    node = 'news' # 크롤링 대상\n",
    "    # search_text = input('검색어를 입력하세요: ')\n",
    "    search_text = '자양강장제품'\n",
    "    cnt = 0\n",
    "    json_result_list = []\n",
    "    json_response = get_naver_search(node, search_text, 1, 100)\n",
    "    while (json_response is not None) and (json_response['display'] != 0):\n",
    "        for post in json_response['items']:\n",
    "            cnt += 1\n",
    "            get_post_data(post, json_result_list, cnt)\n",
    "        start = json_response['start'] + json_response['display']\n",
    "        json_response = get_naver_search(node, search_text, start, 100)\n",
    "    print(f'전체 검색 수: {cnt}')\n",
    "    # csv 파일로 저장\n",
    "    # ['번호', '날짜', '제목', '개요', '원본기사링크', '네이버링크']\n",
    "    columns = ['count', 'date', 'title', 'description', 'org_link', 'link']\n",
    "    result_df = pd.DataFrame(json_result_list, columns=columns)\n",
    "    result_df.to_csv(f'{search_text}_naver_{node}.csv', index=False, encoding='utf-8')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
